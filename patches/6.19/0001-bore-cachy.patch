diff --git a/include/linux/sched/bore.h b/include/linux/sched/bore.h
--- a/include/linux/sched/bore.h	1970-01-01 08:00:00.000000000 +0800
+++ b/include/linux/sched/bore.h	2026-01-05 15:55:40.631603538 +0800
@@ -0,0 +1,39 @@
+#ifndef _KERNEL_SCHED_BORE_H
+#define _KERNEL_SCHED_BORE_H
+
+#include <linux/sched.h>
+#include <linux/sched/cputime.h>
+#include <linux/atomic.h>
+#include <linux/list.h>
+#include <linux/rcupdate.h>
+
+#define SCHED_BORE_AUTHOR   "Masahito Suzuki"
+#define SCHED_BORE_PROGNAME "BORE CPU Scheduler modification"
+
+#define SCHED_BORE_VERSION  "6.5.9"
+
+extern u8   __read_mostly sched_bore;
+extern u8   __read_mostly sched_burst_inherit_type;
+
+extern int sched_burst_cache_lifetime;
+extern int sched_burst_cache_lifetime_disabled;
+extern int sched_burst_cache_miss;
+extern int sched_burst_cache_miss_disabled;
+extern int sched_burst_penalty_offset;
+extern int sched_burst_penalty_scale;
+extern int sched_burst_smoothness_long;
+extern int sched_burst_smoothness_short;
+extern int sched_burst_score_degrade;
+extern int sched_burst_score_degrade_long;
+
+void sched_update_min_base_slice(void);
+void sched_init_bore(void);
+void task_fork_bore(struct task_struct *p, struct task_struct *cur, unsigned long clone_flags,
+		       u64 clone_start_time);
+
+extern void bore_fork(struct task_struct *p, struct task_struct *cur, u64 clone_start_time);
+extern void bore_init_fair_cfs_rq(struct cfs_rq *cfs_rq);
+extern void bore_init_fair_task(struct task_struct *p);
+extern void bore_dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags);
+extern void bore_enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags);
+extern void bore_fair_task_switch(struct task_struct *p);
+extern void bore_task_change_group(struct task_struct *p);
+
+#endif /* _KERNEL_SCHED_BORE_H */

diff --git a/include/linux/sched.h b/include/linux/sched.h
--- a/include/linux/sched.h	2026-01-05 06:41:55.000000000 +0800
+++ b/include/linux/sched.h	2026-01-05 15:55:40.631389633 +0800
@@ -816,6 +816,32 @@ struct kmap_ctrl {
 #endif
 };
 
+#ifdef CONFIG_SCHED_BORE
+#define BORE_BC_TIMESTAMP_SHIFT 16
+
+struct bore_bc {
+	u64				timestamp:	48;
+	u64				penalty:	16;
+};
+
+struct bore_ctx {
+	struct bore_bc	subtree;
+	struct bore_bc	group;
+	u64				burst_time;
+	u16				prev_penalty;
+	u16				curr_penalty;
+	union {
+		u16			penalty;
+		struct {
+			u8		_;
+			u8		score;
+		};
+	};
+	bool			stop_update;
+	bool			futex_waiting;
+};
+#endif /* CONFIG_SCHED_BORE */
+
 struct task_struct {
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/*
@@ -874,6 +900,9 @@ struct task_struct {
 #ifdef CONFIG_SCHED_CLASS_EXT
 	struct sched_ext_entity		scx;
 #endif
+#ifdef CONFIG_SCHED_BORE
+	struct bore_ctx			bore;
+#endif /* CONFIG_SCHED_BORE */
 	const struct sched_class	*sched_class;
 
 #ifdef CONFIG_SCHED_CORE

diff --git a/init/Kconfig b/init/Kconfig
--- a/init/Kconfig	2026-01-05 06:41:55.000000000 +0800
+++ b/init/Kconfig	2026-01-05 15:55:40.631819863 +0800
@@ -1450,6 +1450,23 @@ config CHECKPOINT_RESTORE
 
 	  If unsure, say N here.
 
+config SCHED_BORE
+	bool "Burst-Oriented Response Enhancer"
+	default y
+	help
+	  In Desktop and Mobile computing, one might prefer interactive
+	  tasks to keep responsive no matter what they run in the background.
+
+	  Enabling this kernel feature modifies the scheduler to discriminate
+	  tasks by their burst time (runtime since it last went sleeping or
+	  yielding state) and prioritize those that run less bursty.
+	  Such tasks usually include window compositor, widgets backend,
+	  terminal emulator, video playback, games and so on.
+	  With a little impact to scheduling fairness, it may improve
+	  responsiveness especially under heavy background workload.
+
+	  If unsure, say Y here.
+
 config SCHED_AUTOGROUP
 	bool "Automatic process group scheduling"
 	select CGROUPS

diff --git a/kernel/Kconfig.hz b/kernel/Kconfig.hz
--- a/kernel/Kconfig.hz	2026-01-05 06:41:55.000000000 +0800
+++ b/kernel/Kconfig.hz	2026-01-05 15:55:40.631924447 +0800
@@ -81,3 +81,20 @@ config HZ
 
 config SCHED_HRTICK
 	def_bool HIGH_RES_TIMERS
+
+config MIN_BASE_SLICE_NS
+	int "Default value for min_base_slice_ns"
+	default 2000000
+	help
+	 The BORE Scheduler automatically calculates the optimal base
+	 slice for the configured HZ using the following equation:
+	 
+	 base_slice_ns =
+	 	1000000000/HZ * DIV_ROUNDUP(min_base_slice_ns, 1000000000/HZ)
+	 
+	 This option sets the default lower bound limit of the base slice
+	 to prevent the loss of task throughput due to overscheduling.
+	 
+	 Setting this value too high can cause the system to boot with
+	 an unnecessarily large base slice, resulting in high scheduling
+	 latency and poor system responsiveness.

diff --git a/kernel/fork.c b/kernel/fork.c
--- a/kernel/fork.c	2026-01-05 06:41:55.000000000 +0800
+++ b/kernel/fork.c	2026-01-05 15:55:40.632075572 +0800
@@ -116,6 +116,10 @@
 /* For dup_mmap(). */
 #include "../mm/internal.h"
 
+#ifdef CONFIG_SCHED_BORE
+#include <linux/sched/bore.h>
+#endif /* CONFIG_SCHED_BORE */
+
 #include <trace/events/sched.h>
 
 #define CREATE_TRACE_POINTS
@@ -2380,6 +2384,10 @@ __latent_entropy struct task_struct *copy_process(
 	 * Need tasklist lock for parent etc handling!
 	 */
 	write_lock_irq(&tasklist_lock);
+#ifdef CONFIG_SCHED_BORE
+	if (likely(p->pid))
+		task_fork_bore(p, current, clone_flags, p->start_time);
+#endif /* CONFIG_SCHED_BORE */
 
 	/* CLONE_PARENT re-uses the old parent */
 	if (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {

diff --git a/kernel/futex/waitwake.c b/kernel/futex/waitwake.c
--- a/kernel/futex/waitwake.c	2026-01-05 06:41:55.000000000 +0800
+++ b/kernel/futex/waitwake.c	2026-01-05 15:55:40.632357379 +0800
@@ -4,6 +4,9 @@
 #include <linux/sched/task.h>
 #include <linux/sched/signal.h>
 #include <linux/freezer.h>
+#ifdef CONFIG_SCHED_BORE
+#include <linux/sched/bore.h>
+#endif /* CONFIG_SCHED_BORE */
 
 #include "futex.h"
 
@@ -355,7 +358,15 @@ void futex_do_wait(struct futex_q *q, struct hrtimer_sleeper *timeout)
 		 * is no timeout, or if it has yet to expire.
 		 */
 		if (!timeout || timeout->task)
+#ifdef CONFIG_SCHED_BORE
+		{
+			current->bore.futex_waiting = true;
+#endif /* CONFIG_SCHED_BORE */
 			schedule();
+#ifdef CONFIG_SCHED_BORE
+			current->bore.futex_waiting = false;
+		}
+#endif /* CONFIG_SCHED_BORE */
 	}
 	__set_current_state(TASK_RUNNING);
 }

diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
--- a/kernel/sched/Makefile	2026-01-05 06:41:55.000000000 +0800
+++ b/kernel/sched/Makefile	2026-01-05 15:55:40.632449536 +0800
@@ -37,3 +37,4 @@ obj-y += core.o
 obj-y += fair.o
 obj-y += build_policy.o
 obj-y += build_utility.o
+obj-$(CONFIG_SCHED_BORE) += bore.o

diff --git a/kernel/sched/bore.c b/kernel/sched/bore.c
--- a/kernel/sched/bore.c	1970-01-01 08:00:00.000000000 +0800
+++ b/kernel/sched/bore.c	2026-01-05 15:55:40.632509301 +0800
@@ -0,0 +1,428 @@
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/sched/bore.h>
+#include <linux/mm.h>
+#include <linux/sysctl.h>
+#include <linux/cpuset.h>
+#include <linux/topology.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+#include <linux/mempolicy.h>
+#include <linux/vmalloc.h>
+#include <linux/seq_file.h>
+#include <linux/sched/debug.h>
+
+enum {
+	BORE_BURST_INHERIT_NONE,
+	BORE_BURST_INHERIT_DIRECT,
+	BORE_BURST_INHERIT_DIRECT_EXCLUSIVE,
+	BORE_BURST_INHERIT_REALTIME,
+	BORE_BURST_INHERIT_BEST,
+};
+
+u8 __read_mostly sched_bore = 1;
+u8 __read_mostly sched_burst_inherit_type = BORE_BURST_INHERIT_BEST;
+
+int sched_burst_cache_lifetime = 40000000;
+int sched_burst_cache_lifetime_disabled = 1000000;
+int sched_burst_cache_miss = 2000000;
+int sched_burst_cache_miss_disabled = 200000;
+int sched_burst_penalty_offset = 22;
+int sched_burst_penalty_scale = 21;
+int sched_burst_smoothness_long = 1;
+int sched_burst_smoothness_short = 0;
+int sched_burst_score_degrade = 0;
+int sched_burst_score_degrade_long = 0;
+
+static const struct ctl_table sched_bore_sysctls[];
+
+static u64 max_burst_penalty(void)
+{
+	return (1ULL << (63 - BORE_BC_TIMESTAMP_SHIFT)) - 1;
+}
+
+static u64 max_burst_time(void)
+{
+	return (1ULL << (63 - BORE_BC_TIMESTAMP_SHIFT)) - 1;
+}
+
+static u64 bore_now(void)
+{
+	return sched_clock();
+}
+
+static u64 calc_burst_cache_lifetime(void)
+{
+	if (!sched_bore)
+		return sched_burst_cache_lifetime_disabled;
+
+	return sched_burst_cache_lifetime;
+}
+
+static u64 calc_burst_cache_miss(void)
+{
+	if (!sched_bore)
+		return sched_burst_cache_miss_disabled;
+
+	return sched_burst_cache_miss;
+}
+
+static u64 calc_burst_penalty(u64 burst_time)
+{
+	return min_t(u64, max_burst_penalty(),
+		     div_u64(burst_time, 1ULL << sched_burst_penalty_offset) *
+		     sched_burst_penalty_scale);
+}
+
+static u64 calc_burst_time(struct task_struct *p, u64 now)
+{
+	return min_t(u64, max_burst_time(),
+		     p->bore.burst_time + now - p->se.exec_start);
+}
+
+static void update_burst_penalty(struct task_struct *p, u64 now)
+{
+	if (!sched_bore)
+		return;
+
+	if (unlikely(p->bore.stop_update))
+		return;
+
+	p->bore.burst_time = calc_burst_time(p, now);
+	p->bore.curr_penalty = calc_burst_penalty(p->bore.burst_time);
+}
+
+static void update_burst_score(struct task_struct *p)
+{
+	if (!sched_bore)
+		return;
+
+	if (unlikely(p->bore.stop_update))
+		return;
+
+	if (p->bore.curr_penalty > p->bore.prev_penalty) {
+		u16 diff = p->bore.curr_penalty - p->bore.prev_penalty;
+		u16 factor = 1 + diff / (1 + sched_burst_smoothness_short);
+
+		if (p->bore.score > factor)
+			p->bore.score -= factor;
+		else
+			p->bore.score = 0;
+	} else {
+		u16 diff = p->bore.prev_penalty - p->bore.curr_penalty;
+		u16 factor = 1 + diff / (1 + sched_burst_smoothness_long);
+
+		if (p->bore.score + factor < 255)
+			p->bore.score += factor;
+		else
+			p->bore.score = 255;
+	}
+}
+
+static void update_burst_cache(struct bore_ctx *bc, u64 now)
+{
+	u64 lifetime = calc_burst_cache_lifetime();
+
+	if (bc->subtree.timestamp + lifetime < now) {
+		bc->subtree.penalty = 0;
+		bc->subtree.timestamp = now;
+	}
+
+	if (bc->group.timestamp + lifetime < now) {
+		bc->group.penalty = 0;
+		bc->group.timestamp = now;
+	}
+}
+
+static u64 calc_burst_inherit_penalty(struct bore_ctx *bc, u64 now)
+{
+	u64 penalty = 0;
+
+	if (bc->subtree.timestamp + calc_burst_cache_miss() > now)
+		penalty = bc->subtree.penalty;
+
+	if (bc->group.timestamp + calc_burst_cache_miss() > now)
+		penalty = max(penalty, (u64)bc->group.penalty);
+
+	return penalty;
+}
+
+static void update_burst_cache_group(struct bore_ctx *bc, u64 penalty, u64 now)
+{
+	if (bc->group.penalty < penalty) {
+		bc->group.penalty = penalty;
+		bc->group.timestamp = now;
+	}
+}
+
+static void update_burst_cache_subtree(struct bore_ctx *bc, u64 penalty, u64 now)
+{
+	if (bc->subtree.penalty < penalty) {
+		bc->subtree.penalty = penalty;
+		bc->subtree.timestamp = now;
+	}
+}
+
+static void update_burst_cache_membership(struct task_struct *p, u64 penalty)
+{
+	struct task_struct *leader = p->group_leader;
+	struct bore_ctx *bc = &leader->bore;
+	struct task_struct *sibling;
+
+	update_burst_cache_group(bc, penalty, bore_now());
+
+	if (leader == p)
+		return;
+
+	update_burst_cache_subtree(bc, penalty, bore_now());
+
+	list_for_each_entry(sibling, &leader->thread_group, thread_group) {
+		if (sibling == p)
+			continue;
+		update_burst_cache_subtree(&sibling->bore, penalty, bore_now());
+	}
+}
+
+static void bore_task_switch(struct task_struct *p)
+{
+	if (!sched_bore)
+		return;
+
+	if (p->bore.futex_waiting)
+		return;
+
+	if (unlikely(p->bore.stop_update))
+		return;
+
+	update_burst_penalty(p, bore_now());
+	update_burst_score(p);
+}
+
+static u64 calc_burst_inherit_penalty_current(struct task_struct *p, u64 now)
+{
+	u64 penalty = calc_burst_inherit_penalty(&p->bore, now);
+
+	if (p->bore.curr_penalty > penalty)
+		penalty = p->bore.curr_penalty;
+
+	return penalty;
+}
+
+static void bore_task_dequeue(struct rq *rq, struct task_struct *p)
+{
+	if (!sched_bore)
+		return;
+
+	if (unlikely(p->bore.stop_update))
+		return;
+
+	update_burst_penalty(p, bore_now());
+	update_burst_score(p);
+	update_burst_cache_membership(p, p->bore.curr_penalty);
+}
+
+static void bore_task_enqueue(struct rq *rq, struct task_struct *p)
+{
+	u64 now;
+	u64 penalty;
+
+	if (!sched_bore)
+		return;
+
+	if (unlikely(p->bore.stop_update))
+		return;
+
+	now = bore_now();
+	update_burst_cache(&p->bore, now);
+	penalty = calc_burst_inherit_penalty_current(p, now);
+
+	if (penalty > p->bore.curr_penalty)
+		p->bore.curr_penalty = penalty;
+	if (penalty > p->bore.prev_penalty)
+		p->bore.prev_penalty = penalty;
+}
+
+static void bore_init_fair(struct task_struct *p)
+{
+	if (!sched_bore)
+		return;
+
+	if (unlikely(p->bore.stop_update))
+		return;
+
+	/* Prevent floating point exception due to division by zero */
+	if (!sched_burst_penalty_scale)
+		sched_burst_penalty_scale = 1;
+}
+
+static void bore_init_entity(struct task_struct *p)
+{
+	u64 now;
+
+	if (!sched_bore)
+		return;
+
+	if (unlikely(p->bore.stop_update))
+		return;
+
+	now = bore_now();
+	update_burst_cache(&p->bore, now);
+	p->bore.curr_penalty = calc_burst_inherit_penalty_current(p, now);
+	p->bore.prev_penalty = p->bore.curr_penalty;
+	update_burst_score(p);
+}
+
+static void bore_init_entity_full(struct task_struct *p)
+{
+	if (!sched_bore)
+		return;
+
+	if (unlikely(p->bore.stop_update))
+		return;
+
+	p->bore.burst_time = 0;
+	p->bore.score = 255;
+	p->bore.stop_update = false;
+	update_burst_cache(&p->bore, bore_now());
+	p->bore.curr_penalty = calc_burst_inherit_penalty_current(p, bore_now());
+	p->bore.prev_penalty = p->bore.curr_penalty;
+}
+
+static void bore_update_se(struct task_struct *p, struct sched_entity *se)
+{
+	if (!sched_bore)
+		return;
+
+	if (unlikely(p->bore.stop_update))
+		return;
+
+	se->burst_score = p->bore.score;
+}
+
+static void bore_dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)
+{
+	if (unlikely(p->bore.stop_update))
+		return;
+
+	if (!sched_bore)
+		return;
+
+	if (!(flags & DEQUEUE_SLEEP))
+		return;
+
+	if (task_is_kthread(p))
+		return;
+
+	if (flags & DEQUEUE_SPECIAL)
+		return;
+
+	if (p->bore.futex_waiting)
+		return;
+
+	bore_task_dequeue(rq, p);
+}
+
+static void bore_enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
+{
+	if (unlikely(p->bore.stop_update))
+		return;
+
+	if (!sched_bore)
+		return;
+
+	if (!(flags & ENQUEUE_WAKEUP))
+		return;
+
+	if (task_is_kthread(p))
+		return;
+
+	if (flags & ENQUEUE_SPECIAL)
+		return;
+
+	if (p->bore.futex_waiting)
+		return;
+
+	bore_task_enqueue(rq, p);
+}
+
+static void bore_fair_task_switch(struct task_struct *p)
+{
+	if (!sched_bore)
+		return;
+
+	if (unlikely(p->bore.stop_update))
+		return;
+
+	bore_task_switch(p);
+}
+
+static void bore_fair_set_task_group(struct task_struct *p)
+{
+	if (!sched_bore)
+		return;
+
+	if (unlikely(p->bore.stop_update))
+		return;
+
+	update_burst_cache_membership(p, p->bore.curr_penalty);
+}
+
+void sched_init_bore(void)
+{
+	if (!sched_bore)
+		return;
+
+	register_sysctl("kernel", sched_bore_sysctls);
+}
+
+void sched_update_min_base_slice(void)
+{
+	if (!sched_bore)
+		return;
+
+	update_sysctl();
+}
+
+void bore_task_change_group(struct task_struct *p)
+{
+	if (!sched_bore)
+		return;
+
+	if (unlikely(p->bore.stop_update))
+		return;
+
+	update_burst_cache_membership(p, p->bore.curr_penalty);
+}
+
+void bore_fork(struct task_struct *p, struct task_struct *cur, u64 clone_start_time)
+{
+	u64 now;
+	u64 penalty;
+
+	if (!sched_bore)
+		return;
+
+	if (unlikely(cur->bore.stop_update))
+		return;
+
+	now = bore_now();
+	update_burst_cache(&cur->bore, now);
+	penalty = calc_burst_inherit_penalty_current(cur, now);
+
+	if (penalty > cur->bore.curr_penalty)
+		cur->bore.curr_penalty = penalty;
+	if (penalty > cur->bore.prev_penalty)
+		cur->bore.prev_penalty = penalty;
+
+	if (p->bore.score > cur->bore.score)
+		p->bore.score = cur->bore.score;
+
+	p->bore.burst_time = 0;
+	update_burst_cache(&p->bore, now);
+	p->bore.curr_penalty = calc_burst_inherit_penalty_current(p, now);
+	p->bore.prev_penalty = p->bore.curr_penalty;
+
+	update_burst_cache_membership(p, p->bore.curr_penalty);
+}
+
+void task_fork_bore(struct task_struct *p, struct task_struct *cur, unsigned long clone_flags,
+		      u64 clone_start_time)
+{
+	if (!sched_bore)
+		return;
+
+	if (unlikely(cur->bore.stop_update))
+		return;
+
+	if (!(clone_flags & CLONE_THREAD)) {
+		bore_fork(p, cur, clone_start_time);
+		return;
+	}
+
+	if (clone_flags & CLONE_THREAD)
+		bore_fork(p, cur, clone_start_time);
+}
+
+void bore_init_fair_cfs_rq(struct cfs_rq *cfs_rq)
+{
+	if (!sched_bore)
+		return;
+
+	if (unlikely(!cfs_rq))
+		return;
+
+	if (unlikely(cfs_rq->bore.stop_update))
+		return;
+
+	cfs_rq->bore.burst_time = 0;
+	cfs_rq->bore.score = 255;
+	cfs_rq->bore.curr_penalty = 0;
+	cfs_rq->bore.prev_penalty = 0;
+	cfs_rq->bore.stop_update = false;
+}
+
+void bore_init_fair_task(struct task_struct *p)
+{
+	if (!sched_bore)
+		return;
+
+	if (unlikely(p->bore.stop_update))
+		return;
+
+	bore_init_entity_full(p);
+	bore_init_entity(p);
+}
+
+void bore_dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)
+{
+	bore_dequeue_task_fair(rq, p, flags);
+}
+
+void bore_enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
+{
+	bore_enqueue_task_fair(rq, p, flags);
+}
+
+void bore_fair_task_switch(struct task_struct *p)
+{
+	bore_fair_task_switch(p);
+}
+
+void bore_task_change_group(struct task_struct *p)
+{
+	bore_task_change_group(p);
+}
+
+void bore_update_fair_task(struct task_struct *p, struct sched_entity *se)
+{
+	if (!sched_bore)
+		return;
+
+	if (unlikely(p->bore.stop_update))
+		return;
+
+	bore_update_se(p, se);
+}
+
+static int sched_bore_show(struct seq_file *m, void *v)
+{
+	SEQ_printf(m, "sched_bore\t: %d\n", sched_bore);
+	SEQ_printf(m, "sched_burst_inherit_type\t: %d\n", sched_burst_inherit_type);
+
+ 	SEQ_printf(m, "sched_burst_cache_lifetime\t: %d\n", sched_burst_cache_lifetime);
+ 	SEQ_printf(m, "sched_burst_cache_lifetime_disabled\t: %d\n", sched_burst_cache_lifetime_disabled);
+ 	SEQ_printf(m, "sched_burst_cache_miss\t: %d\n", sched_burst_cache_miss);
+ 	SEQ_printf(m, "sched_burst_cache_miss_disabled\t: %d\n", sched_burst_cache_miss_disabled);
+ 	SEQ_printf(m, "sched_burst_penalty_offset\t: %d\n", sched_burst_penalty_offset);
+ 	SEQ_printf(m, "sched_burst_penalty_scale\t: %d\n", sched_burst_penalty_scale);
+ 	SEQ_printf(m, "sched_burst_smoothness_long\t: %d\n", sched_burst_smoothness_long);
+ 	SEQ_printf(m, "sched_burst_smoothness_short\t: %d\n", sched_burst_smoothness_short);
+ 	SEQ_printf(m, "sched_burst_score_degrade\t: %d\n", sched_burst_score_degrade);
+ 	SEQ_printf(m, "sched_burst_score_degrade_long\t: %d\n", sched_burst_score_degrade_long);
+
+ 	SEQ_printf(m, "sched_burst_penalty_offset\t: %d\n", sched_burst_penalty_offset);
+ 	SEQ_printf(m, "sched_burst_penalty_scale\t: %d\n", sched_burst_penalty_scale);
+ 	SEQ_printf(m, "sched_burst_smoothness_long\t: %d\n", sched_burst_smoothness_long);
+ 	SEQ_printf(m, "sched_burst_smoothness_short\t: %d\n", sched_burst_smoothness_short);
+ 	SEQ_printf(m, "sched_burst_score_degrade\t: %d\n", sched_burst_score_degrade);
+ 	SEQ_printf(m, "sched_burst_score_degrade_long\t: %d\n", sched_burst_score_degrade_long);
+
+	return 0;
+}
+
+static int sched_bore_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, sched_bore_show, NULL);
+}
+
+static const struct file_operations sched_bore_fops = {
+	.open		= sched_bore_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int __init sched_bore_init(void)
+{
+	if (!sched_bore)
+		return 0;
+
+	debugfs_create_file("sched_bore", 0444, NULL, NULL, &sched_bore_fops);
+	return 0;
+}
+late_initcall(sched_bore_init);
+
+static const struct ctl_table sched_bore_sysctls[] = {
+	{
+		.procname	= "sched_bore",
+		.data		= &sched_bore,
+		.maxlen		= sizeof(sched_bore),
+		.mode		= 0644,
+		.proc_handler	= proc_dou8vec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_ONE,
+	},
+	{
+		.procname	= "sched_burst_inherit_type",
+		.data		= &sched_burst_inherit_type,
+		.maxlen		= sizeof(sched_burst_inherit_type),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_ONE,
+	},
+	{
+		.procname	= "sched_burst_cache_lifetime",
+		.data		= &sched_burst_cache_lifetime,
+		.maxlen		= sizeof(sched_burst_cache_lifetime),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_THOUSAND,
+	},
+	{
+		.procname	= "sched_burst_cache_lifetime_disabled",
+		.data		= &sched_burst_cache_lifetime_disabled,
+		.maxlen		= sizeof(sched_burst_cache_lifetime_disabled),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_THOUSAND,
+	},
+	{
+		.procname	= "sched_burst_cache_miss",
+		.data		= &sched_burst_cache_miss,
+		.maxlen		= sizeof(sched_burst_cache_miss),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_THOUSAND,
+	},
+	{
+		.procname	= "sched_burst_cache_miss_disabled",
+		.data		= &sched_burst_cache_miss_disabled,
+		.maxlen		= sizeof(sched_burst_cache_miss_disabled),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_THOUSAND,
+	},
+	{
+		.procname	= "sched_burst_penalty_offset",
+		.data		= &sched_burst_penalty_offset,
+		.maxlen		= sizeof(sched_burst_penalty_offset),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_THOUSAND,
+	},
+	{
+		.procname	= "sched_burst_penalty_scale",
+		.data		= &sched_burst_penalty_scale,
+		.maxlen		= sizeof(sched_burst_penalty_scale),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_THOUSAND,
+	},
+	{
+		.procname	= "sched_burst_smoothness_long",
+		.data		= &sched_burst_smoothness_long,
+		.maxlen		= sizeof(sched_burst_smoothness_long),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_THOUSAND,
+	},
+	{
+		.procname	= "sched_burst_smoothness_short",
+		.data		= &sched_burst_smoothness_short,
+		.maxlen		= sizeof(sched_burst_smoothness_short),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_THOUSAND,
+	},
+	{
+		.procname	= "sched_burst_score_degrade",
+		.data		= &sched_burst_score_degrade,
+		.maxlen		= sizeof(sched_burst_score_degrade),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_THOUSAND,
+	},
+	{
+		.procname	= "sched_burst_score_degrade_long",
+		.data		= &sched_burst_score_degrade_long,
+		.maxlen		= sizeof(sched_burst_score_degrade_long),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_THOUSAND,
+	},
+	{
+		.procname	= "sched_burst_cache_lifetime",
+		.data		= &sched_burst_cache_lifetime,
+		.maxlen		= sizeof(sched_burst_cache_lifetime),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_THOUSAND,
+	},
+	{}
+};
+
+int __init bore_init_fair(unsigned int cpu)
+{
+	struct cfs_rq *cfs_rq = &cpu_rq(cpu)->cfs;
+	
+	if (unlikely(!cfs_rq))
+		return 0;
+
+	bore_init_fair_cfs_rq(cfs_rq);
+	return 0;
+}
+late_initcall(bore_init_fair);
+
+int __init bore_init_task(void)
+{
+	struct task_struct *p = &init_task;
+
+	bore_init_fair_task(p);
+	return 0;
+}
+late_initcall(bore_init_task);
+
+MODULE_AUTHOR(SCHED_BORE_AUTHOR);
+MODULE_DESCRIPTION(SCHED_BORE_PROGNAME);
+MODULE_LICENSE("GPL");

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
--- a/kernel/sched/core.c	2026-01-05 06:41:55.000000000 +0800
+++ b/kernel/sched/core.c	2026-01-05 15:55:40.632950732 +0800
@@ -100,6 +100,10 @@
 #include "../smpboot.h"
 #include "../locking/mutex.h"
 
+#ifdef CONFIG_SCHED_BORE
+#include <linux/sched/bore.h>
+#endif /* CONFIG_SCHED_BORE */
+
 EXPORT_TRACEPOINT_SYMBOL_GPL(ipi_send_cpu);
 EXPORT_TRACEPOINT_SYMBOL_GPL(ipi_send_cpumask);

